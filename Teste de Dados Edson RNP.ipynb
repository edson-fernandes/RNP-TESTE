{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123effde-1436-4ee3-befd-7f1af669c50a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Criando conexão com banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b689aeb6-37d8-4f67-a591-a4f76901f2eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-----------------+------------------+----------+--------------+-------------+------+--------+-----------+-------+-------------------------+------------+\n|customer_number|customer_name|contact_last_name|contact_first_name|     phone| address_line1|address_line2|  city|   state|postal_code|country|sales_rep_employee_number|credit_limit|\n+---------------+-------------+-----------------+------------------+----------+--------------+-------------+------+--------+-----------+-------+-------------------------+------------+\n|            103|         Jake|             King|           Carine |40.32.2555|54, rue Royale|         null|Nantes|Victoria|      44000| France|                     1370|    21000.00|\n+---------------+-------------+-----------------+------------------+----------+--------------+-------------+------+--------+-----------+-------+-------------------------+------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL reader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Detalhes da conexão\n",
    "database_url = \"jdbc:postgresql://psql-mock-database-cloud.postgres.database.azure.com:5432/ecom1692884890642lsbtqzttgrtzvnoc\"\n",
    "properties = {\n",
    "    \"user\": \"qvwyyojejjekojytdvfmnsfk@psql-mock-database-cloud\",\n",
    "    \"password\": \"pemhzteldibgloibpdqjdccq\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "#Função para ler tabelas\n",
    "def read_table(table_name):\n",
    "    return spark.read.jdbc(database_url, table_name, properties=properties)\n",
    "\n",
    "# lendo as tabelas\n",
    "customers_df = read_table(\"customers\")\n",
    "employees_df = read_table(\"employees\")\n",
    "offices_df = read_table(\"offices\")\n",
    "orderdetails_df = read_table(\"orderdetails\")\n",
    "orders_df = read_table(\"orders\")\n",
    "payments_df = read_table(\"payments\")\n",
    "product_lines_df = read_table(\"product_lines\")\n",
    "products_df = read_table(\"products\")\n",
    "\n",
    "#Verifica se os dados estão carregados\n",
    "customers_df.show(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a2b967-e27d-49be-8a0b-b7bc3a7518b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#criando a lógica para salvar cada tabela como parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362c4dbb-24a5-4b29-9606-c7b9f96d4957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criando caminho para salvar as tabelas em parquet\n",
    "base_path = \"/edson_arquivos_parquet/\"\n",
    "\n",
    "#Função para salvar DataFrame como Parquet\n",
    "def save_as_parquet(df, table_name):\n",
    "    parquet_path = base_path + table_name + \".parquet\"\n",
    "    df.write.mode('overwrite').parquet(parquet_path)\n",
    "\n",
    "# Salva cada tabela como um arquivo Parquet\n",
    "save_as_parquet(customers_df, \"customers\")\n",
    "save_as_parquet(employees_df, \"employees\")\n",
    "save_as_parquet(offices_df, \"offices\")\n",
    "save_as_parquet(orderdetails_df, \"orderdetails\")\n",
    "save_as_parquet(orders_df, \"orders\")\n",
    "save_as_parquet(payments_df, \"payments\")\n",
    "save_as_parquet(product_lines_df, \"product_lines\")\n",
    "save_as_parquet(products_df, \"products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6492d2a-40be-4df2-9fd3-1108522b68ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# criando tabelas temporarias, para montar as consulta em SQL\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "offices_df.createOrReplaceTempView(\"offices\")\n",
    "payments_df.createOrReplaceTempView(\"payments\")\n",
    "product_lines_df.createOrReplaceTempView(\"product_lines\")\n",
    "orderdetails_df.createOrReplaceTempView(\"orderdetails\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8875b56f-7198-4a23-b03d-5b803be5ea7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Qual país possui a maior quantidade de itens cancelados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cca83d-3d1b-484e-846c-aa8bbc5a5103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|country|total_cancelled|\n+-------+---------------+\n|  Spain|            605|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "result1 = spark.sql(\"\"\"\n",
    "      SELECT c.country, SUM(od.quantity_ordered) AS total_cancelled\n",
    "    FROM orders o\n",
    "    JOIN orderdetails od ON o.order_number = od.order_number\n",
    "    JOIN customers c ON o.customer_number = c.customer_number\n",
    "    WHERE o.status = 'Cancelled'\n",
    "    GROUP BY c.country\n",
    "    ORDER BY total_cancelled DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "result1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fae67b-49a6-49de-a2c5-1b410a3334a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Qual o faturamento da linha de produto mais vendido, considere como os itens Shipped, cujo o pedido foi realizado no ano de 2005?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13804bb-c93d-4e18-837c-d75ba504f7e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n|product_line|total_revenue|\n+------------+-------------+\n|Classic Cars|     16220.62|\n+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT p.product_line, SUM(od.price_each) AS total_revenue\n",
    "    FROM orders o\n",
    "    JOIN orderdetails od ON o.order_number = od.order_number\n",
    "    JOIN products p ON od.product_code = p.product_code\n",
    "    WHERE o.status = 'Shipped' AND YEAR(o.order_date) = 2005\n",
    "    GROUP BY p.product_line\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "result2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a995a68f-8f9a-48a9-89f6-3d176109da22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Nome, sobrenome e e-mail dos vendedores do Japão, o local-part do e-mail deve estar mascarado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb97659-52bf-48b3-b155-a2636ec480e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n|first_name|last_name|        masked_email|\n+----------+---------+--------------------+\n|      Mami|    Nishi|******@classicmod...|\n|   Yoshimi|     Kato|*****@classicmode...|\n+----------+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "result3 = spark.sql(\"\"\"\n",
    "    SELECT e.first_name, e.last_name, \n",
    "           CONCAT(REPEAT('*', LENGTH(SPLIT(e.email, '@')[0])), '@', SPLIT(e.email, '@')[1]) as masked_email\n",
    "    FROM employees e\n",
    "    JOIN offices o ON e.office_code = o.office_code\n",
    "    WHERE o.country = 'Japan'\n",
    "\"\"\")\n",
    "result3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6b76b6-f8fd-4b21-af17-f719d408ec77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#No notebook crie um merge entre a tabela JDBC e os arquivos parquet, o merge \n",
    "###deve conter a lógica de insert, update e delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f965b5-baa6-4f68-a6e7-27592cf0180b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe(target_df, reference_df):\n",
    "    \"\"\"\n",
    "    Convertendo colunas de target_df para corresponder aos tipos de dados de reference_df.\n",
    "    Argumentos:\n",
    "    - target_df (DataFrame): DataFrame cujas colunas precisam ser convertidas.\n",
    "    - reference_df (DataFrame): DataFrame utilizado como referência para o casting.\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame: target_df após a conversão.\n",
    "    \"\"\"\n",
    "    for column in target_df.columns:\n",
    "        ref_column = column.replace(\"parquet_\", \"\")\n",
    "        target_df = target_df.withColumn(column, target_df[column].cast(reference_df.schema[ref_column].dataType))\n",
    "    return target_df\n",
    "\n",
    "def upsert_table(jdbc_df, parquet_df, primary_keys):\n",
    "    # Gerando hashes para jdbc_df\n",
    "    hash_expression = md5(concat_ws(\"|\", *jdbc_df.columns)).alias(\"row_hash\")\n",
    "    jdbc_df = jdbc_df.withColumn(\"row_hash\", hash_expression)\n",
    "\n",
    "    # Gerando hashes para parquet_df\n",
    "    hash_expression = md5(concat_ws(\"|\", *parquet_df.columns)).alias(\"row_hash\")\n",
    "    parquet_df = parquet_df.withColumn(\"row_hash\", hash_expression)\n",
    "\n",
    "    # Identificando registros novos e atualizados\n",
    "    new_records = parquet_df.join(jdbc_df, on=[\"row_hash\"], how=\"left_anti\")\n",
    "    updated_records = parquet_df.join(jdbc_df, on=[\"row_hash\"], how=\"inner\")\n",
    "\n",
    "    # Identificando registros excluídos\n",
    "    deleted_records_hash = jdbc_df.join(parquet_df, on=[\"row_hash\"], how=\"left_anti\").select(\"row_hash\")\n",
    "\n",
    "    # Seleção explícita das colunas de parquet_df\n",
    "    columns_to_select = [f\"parquet_{col}\" for col in jdbc_df.columns if col != \"row_hash\"]\n",
    "    updated_records = updated_records.select(*columns_to_select)\n",
    "    new_records = new_records.select(*columns_to_select)\n",
    "\n",
    "    # Casting\n",
    "    new_records = cast_dataframe(new_records, jdbc_df)\n",
    "    updated_records = cast_dataframe(updated_records, jdbc_df)\n",
    "\n",
    "    # Excluindo registros deletados\n",
    "    result_df = jdbc_df.join(deleted_records_hash, on=\"row_hash\", how=\"left_anti\").drop(\"row_hash\")\n",
    "\n",
    "    # Adicionando registros atualizados e novos\n",
    "    return result_df.union(new_records).union(updated_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510acb24-4126-4e6e-8c34-324f66881f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Upsert\").getOrCreate()\n",
    "from pyspark.sql.functions import col, md5, concat_ws\n",
    "\n",
    "\n",
    "\n",
    "def rename_parquet_columns(df):\n",
    "    return df.select([col(column).alias(f\"parquet_{column}\") for column in df.columns])\n",
    "\n",
    "\n",
    "# Renomeando as colunas em parquet_df antes da junção\n",
    "def rename_parquet_columns(df):\n",
    "    return df.select([col(column).alias(f\"parquet_{column}\") for column in df.columns])\n",
    "\n",
    "# Para a tabela customers\n",
    "jdbc_df = spark.sql('SELECT * FROM customers')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/customers.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_customers_df = upsert_table(jdbc_df, parquet_df, [\"parquet_customer_number\"])\n",
    "\n",
    "# Para a tabela employees\n",
    "jdbc_df = spark.sql('SELECT * FROM employees')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/employees.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_employee_df = upsert_table(jdbc_df, parquet_df, [\"parquet_employee_number\"])\n",
    "\n",
    "# Para a tabela offices\n",
    "jdbc_df = spark.sql('SELECT * FROM offices')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/offices.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_office_df = upsert_table(jdbc_df, parquet_df, [\"parquet_office_code\"])\n",
    "\n",
    "# Para a tabela orderdetails\n",
    "jdbc_df = spark.sql('SELECT * FROM orderdetails')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/orderdetails.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_orderdetails_df = upsert_table(jdbc_df, parquet_df, [\"parquet_order_number\", \"parquet_product_code\"])\n",
    "\n",
    "# Para a tabela orders\n",
    "jdbc_df = spark.sql('SELECT * FROM orders')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/orders.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_order_df = upsert_table(jdbc_df, parquet_df, [\"parquet_order_number\"])\n",
    "\n",
    "# Para a tabela payments\n",
    "jdbc_df = spark.sql('SELECT * FROM payments')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/payments.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_payments_df = upsert_table(jdbc_df, parquet_df, [\"parquet_customer_number\", \"parquet_check_number\"])\n",
    "\n",
    "# Para a tabela product_lines\n",
    "jdbc_df = spark.sql('SELECT * FROM product_lines')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/product_lines.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_product_lines_df = upsert_table(jdbc_df, parquet_df, [\"parquet_product_line\"])\n",
    "\n",
    "# Para a tabela products\n",
    "jdbc_df = spark.sql('SELECT * FROM products')\n",
    "parquet_df = spark.read.parquet(\"/edson_arquivos_parquet/products.parquet\")\n",
    "parquet_df = rename_parquet_columns(parquet_df)\n",
    "merged_products_df = upsert_table(jdbc_df, parquet_df, [\"parquet_product_code\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f151ea-0082-46ce-966d-621d86124cea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Salve os resultados em formato delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55257939-f164-4d76-baac-23a7d408b0cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Para o dataframe merged_customers_df\n",
    "path_customers = \"/edson_arquivos_delta/customers\"\n",
    "merged_customers_df.write.format(\"delta\").mode(\"overwrite\").save(path_customers)\n",
    "\n",
    "# Para o dataframe merged_employee_df\n",
    "path_employee = \"/edson_arquivos_delta/employee\"\n",
    "merged_employee_df.write.format(\"delta\").mode(\"overwrite\").save(path_employee)\n",
    "\n",
    "# Para o dataframe merged_office_df\n",
    "path_office = \"/edson_arquivos_delta/office\"\n",
    "merged_office_df.write.format(\"delta\").mode(\"overwrite\").save(path_office)\n",
    "\n",
    "# Para o dataframe merged_orderdetails_df\n",
    "path_orderdetails = \"/edson_arquivos_delta/orderdetails\"\n",
    "merged_orderdetails_df.write.format(\"delta\").mode(\"overwrite\").save(path_orderdetails)\n",
    "\n",
    "# Para o dataframe merged_order_df\n",
    "path_order = \"/edson_arquivos_delta/order\"\n",
    "merged_order_df.write.format(\"delta\").mode(\"overwrite\").save(path_order)\n",
    "\n",
    "# Para o dataframe merged_payments_df\n",
    "path_payments = \"/edson_arquivos_delta/payments\"\n",
    "merged_payments_df.write.format(\"delta\").mode(\"overwrite\").save(path_payments)\n",
    "\n",
    "# Para o dataframe merged_product_lines_df\n",
    "path_product_lines = \"/edson_arquivos_delta/product_lines\"\n",
    "merged_product_lines_df.write.format(\"delta\").mode(\"overwrite\").save(path_product_lines)\n",
    "\n",
    "# Para o dataframe merged_products_df\n",
    "path_products = \"/edson_arquivos_delta/products\"\n",
    "merged_products_df.write.format(\"delta\").mode(\"overwrite\").save(path_products)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2122547336770787,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Teste de Dados Edson RNP",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
